{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmVlMKzz76kIY/BPsuV1zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/iNeauron_FSDS_Assignments/blob/main/Computer_vision/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. After each stride-2 conv, why do we double the number of filters?\n",
        "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
        "3. What data is saved by ActivationStats for each layer?\n",
        "\n",
        "4. How do we get a learner&#39;s callback after they&#39;ve completed training?\n",
        "\n",
        "5. What are the drawbacks of activations above zero?\n",
        "\n",
        "6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
        "\n",
        "7. Why should we avoid starting training with a high learning rate?\n",
        "\n",
        "8. What are the pros of studying with a high rate of learning?\n",
        "\n",
        "9. Why do we want to end the training with a low learning rate?"
      ],
      "metadata": {
        "id": "zieG8-wwfkqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer"
      ],
      "metadata": {
        "id": "20tG0DAMfmSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Doubling the number of filters after each stride-2 convolutional layer is a common practice in convolutional neural networks (CNNs) to increase the network's capacity and enable it to learn more complex and diverse features. When applying a stride-2 convolution, the spatial dimensions of the feature maps are reduced by half, while the number of channels (filters) remains the same. By doubling the number of filters, the network can compensate for the reduction in spatial dimensions and capture more distinctive features in the deeper layers.\n",
        "\n",
        "2. When using a simple CNN architecture for the MNIST dataset, a larger kernel size (e.g., 5x5) is used in the first convolutional layer to capture more spatial context and capture larger patterns or features in the input images. MNIST images are relatively small (28x28 pixels), and using a larger kernel allows the network to have a larger receptive field, enabling it to capture more information and higher-level features in a single convolutional operation.\n",
        "\n",
        "3. ActivationStats saves the following data for each layer during training:\n",
        "\n",
        "   - Activations: It records the activation values for each input sample and each neuron in the layer. This allows analyzing the distribution and statistics of activations over the training process, such as mean, standard deviation, and histograms.\n",
        "   \n",
        "   - Gradients: It tracks the gradients of the layer's parameters during backpropagation. This information helps in understanding the flow of gradients and identifying potential issues like vanishing or exploding gradients.\n",
        "   \n",
        "   - Stats: It records statistical measures such as mean, standard deviation, and histograms of activations and gradients. These statistics provide insights into the behavior and characteristics of the layer throughout the training.\n",
        "\n",
        "4. To get a learner's callback after they have completed training, you can use the `after_fit` callback in the fastai library. This callback is executed after the completion of the training loop and provides access to the learner object, allowing you to perform additional actions or analysis.\n",
        "\n",
        "You can define a custom callback class that inherits from `Callback` and override the `after_fit` method. Inside this method, you can access the learner object and perform any desired actions based on the trained model or training statistics.\n",
        "\n",
        "5. Drawbacks of activations above zero (positive values) can include:\n",
        "\n",
        "   - Saturation: If activations become too large, they can saturate non-linear activation functions (e.g., sigmoid or tanh), leading to vanishing gradients and limiting the network's ability to learn effectively.\n",
        "   \n",
        "   - Unbounded growth: In some cases, activations that are not properly bounded can grow exponentially, resulting in unstable training and numerical issues.\n",
        "   \n",
        "   - Overfitting: Very large activations can lead to overfitting, where the model becomes too specialized to the training data and performs poorly on unseen data.\n",
        "   \n",
        "   - Increased memory and computation requirements: Large activations require more memory and computational resources, making the network more resource-intensive and potentially slower to train and deploy.\n",
        "\n",
        "6. Benefits of training with larger batches:\n",
        "\n",
        "   - Improved training efficiency: Larger batch sizes allow for more efficient use of parallel processing capabilities, especially when training on GPUs. It can speed up the training process by performing computations on multiple examples simultaneously.\n",
        "   \n",
        "   - Smoother convergence: Larger batch sizes can lead to more stable gradients and smoother convergence, as the noise introduced by individual examples is averaged out.\n",
        "   \n",
        "   - More accurate gradient estimates: With larger batches, the gradients calculated from the batch can provide more accurate estimates of the overall direction of the loss landscape, reducing the impact of noisy gradients.\n",
        "   \n",
        "   Drawbacks of training with larger batches:\n",
        "   \n",
        "   - Increased memory requirements: Larger batch sizes require more memory to store the activations and gradients during training. This can be a limitation, especially when training on resource-constrained devices.\n",
        "   \n",
        "   - Generalization limitations\n",
        "\n",
        ": Training with larger batches may result in poorer generalization performance, as the model may not be exposed to a diverse range of examples within each batch, leading to biased gradients and suboptimal solutions.\n",
        "   \n",
        "   - Difficulty in escaping local minima: Larger batches can make it more challenging for the optimization process to escape from local minima, as the model may get stuck in regions with higher loss due to the reduced exploration of the loss landscape.\n",
        "\n",
        "7. Starting training with a high learning rate can be problematic because it can lead to unstable training and make it difficult for the model to converge to a good solution. Some reasons to avoid a high learning rate at the beginning of training include:\n",
        "\n",
        "   - Overshooting: A high learning rate can cause the model's parameters to update too aggressively, leading to overshooting the optimal solution and bouncing around in the parameter space.\n",
        "   \n",
        "   - Divergence: A very high learning rate can cause the loss to increase instead of decreasing during training. This divergence makes it challenging for the model to learn meaningful patterns and negatively affects the training process.\n",
        "   \n",
        "   - Difficulty in finding the global minimum: Starting with a high learning rate may prevent the model from effectively exploring the loss landscape and finding the global minimum. It can result in the model getting trapped in local minima or suboptimal regions.\n",
        "   \n",
        "8. Pros of training with a high learning rate:\n",
        "\n",
        "   - Faster convergence: A high learning rate can lead to faster initial progress during training, as the model updates its parameters more significantly in each iteration.\n",
        "   \n",
        "   - Escaping local minima: In some cases, a higher learning rate can help the model escape from local minima and explore a wider range of the loss landscape.\n",
        "   \n",
        "   - Exploration of diverse solutions: A high learning rate encourages the model to explore different areas of the parameter space, potentially leading to the discovery of novel and useful solutions.\n",
        "   \n",
        "   However, it is important to note that training with a high learning rate requires careful tuning and monitoring, as it can also lead to unstable training and poor convergence if not controlled properly.\n",
        "\n",
        "9. Ending the training with a low learning rate is beneficial for fine-tuning the model and achieving better optimization near the end of the training process. Some reasons to lower the learning rate towards the end of training include:\n",
        "\n",
        "   - Refinement of learned features: A lower learning rate allows the model to fine-tune the learned features and adjust the parameters with smaller updates. This can help the model converge to a better and more refined solution.\n",
        "   \n",
        "   - Smoother convergence: Decreasing the learning rate towards the end of training helps the model to converge more smoothly and improve its generalization performance.\n",
        "   \n",
        "   - Preventing overfitting: A lower learning rate can act as a regularization technique, preventing overfitting by reducing the magnitude of parameter updates and encouraging the model to find a more general solution.\n",
        "   \n",
        "   Lowering the learning rate gradually or using learning rate schedules, such as learning rate decay or learning rate annealing, can help the model reach a stable and optimal solution towards the end of training."
      ],
      "metadata": {
        "id": "9N18YcbFfoac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXZ_oWHmb1F-"
      },
      "outputs": [],
      "source": []
    }
  ]
}