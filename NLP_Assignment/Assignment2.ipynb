{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmU4v5dmiOyiM7GGA9PO/L"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Corpora?\n",
        "2. What are Tokens?\n",
        "3. What are Unigrams, Bigrams, Trigrams?\n",
        "4. How to generate n-grams from text?\n",
        "5. Explain Lemmatization\n",
        "6. Explain Stemming\n",
        "7. Explain Part-of-speech (POS) tagging\n",
        "8. Explain Chunking or shallow parsing\n",
        "9. Explain Noun Phrase (NP) chunking\n",
        "10. Explain Named Entity Recognition"
      ],
      "metadata": {
        "id": "zC_jmnBQvZG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y2L41zYmvWut"
      },
      "outputs": [],
      "source": [
        "# Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Corpora (singular: corpus) refer to a collection of texts or documents that are used for the purpose of analysis, research, or study. These texts or documents can be in any format, such as books, articles, newspapers, speeches, tweets, or even chat logs.\n",
        "\n",
        "2. Tokens are the basic units of text, which are typically individual words or punctuation marks. Tokenization is the process of breaking down a text into tokens.\n",
        "\n",
        "3. Unigrams, Bigrams, and Trigrams are types of n-grams, which are contiguous sequences of n items from a given sample of text. Unigrams refer to single words, Bigrams refer to sequences of two words, and Trigrams refer to sequences of three words.\n",
        "\n",
        "4. To generate n-grams from text, we can use the ngrams function from the nltk library in Python. For example, the following code generates all trigrams from a given sentence:\n",
        "\n"
      ],
      "metadata": {
        "id": "k9nfFiEWveE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fN96E5Zv0Ng",
        "outputId": "79389017-e459-44f4-e0cc-23a3f7076b14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "trigrams = nltk.ngrams(tokens, 3)\n",
        "\n",
        "for trigram in trigrams:\n",
        "    print(trigram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN4SL8iPvdju",
        "outputId": "92c9b7fd-413c-4917-b26e-2650fb8b1f48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The', 'quick', 'brown')\n",
            "('quick', 'brown', 'fox')\n",
            "('brown', 'fox', 'jumps')\n",
            "('fox', 'jumps', 'over')\n",
            "('jumps', 'over', 'the')\n",
            "('over', 'the', 'lazy')\n",
            "('the', 'lazy', 'dog')\n",
            "('lazy', 'dog', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Lemmatization is the process of reducing words to their base or dictionary form, which is known as the lemma. The purpose of lemmatization is to group together different forms of the same word, such as \"run\", \"running\", and \"ran\", so that they can be analyzed or processed as a single item.\n",
        "\n",
        "6. Stemming is the process of reducing words to their root or stem form by removing suffixes and prefixes. The purpose of stemming is to normalize words so that variations of the same word can be grouped together, such as \"run\", \"running\", and \"ran\".\n",
        "\n",
        "7. Part-of-speech (POS) tagging is the process of labeling each word in a text with its corresponding part of speech, such as noun, verb, adjective, or adverb. POS tagging is often used as a preliminary step in text analysis or natural language processing.\n",
        "\n",
        "8. Chunking, also known as shallow parsing, is the process of grouping together adjacent words in a sentence to form larger syntactic units, such as noun phrases, verb phrases, or clauses. Chunking is often used to extract information from text and to identify patterns or structures within text.\n",
        "\n",
        "9. Noun Phrase (NP) chunking is a specific type of chunking that focuses on identifying and grouping together noun phrases in a sentence. Noun phrases typically consist of a noun and any associated modifiers, such as adjectives, prepositions, or determiners.\n",
        "\n",
        "10. Named Entity Recognition (NER) is the process of identifying and extracting named entities from text, such as people, organizations, locations, or dates. NER is often used in information extraction and natural language processing applications, such as chatbots, search engines, and recommender systems."
      ],
      "metadata": {
        "id": "-Sqzjkvyv_Ni"
      }
    }
  ]
}