{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDmbQeTkxLKIP1qsHPs5M1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/iNeauron_FSDS_Assignments/blob/main/NLP_Assignment/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Vanilla autoencoders\n",
        "2. What are Sparse autoencoders\n",
        "3. What are Denoising autoencoders\n",
        "4. What are Convolutional autoencoders\n",
        "5. What are Stacked autoencoders\n",
        "6. Explain how to generate sentences using LSTM autoencoders\n",
        "7. Explain Extractive summarization\n",
        "8. Explain Abstractive summarization\n",
        "9. Explain Beam search\n",
        "10. Explain Length normalization\n",
        "11. Explain Coverage normalization\n",
        "12. Explain ROUGE metric evaluation"
      ],
      "metadata": {
        "id": "G-HdgGN5apyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer"
      ],
      "metadata": {
        "id": "8JefSzynargU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Vanilla autoencoders, also known as traditional or basic autoencoders, are a type of neural network architecture used for unsupervised learning and data compression. They consist of an encoder network and a decoder network. The encoder network takes an input data sample and maps it to a lower-dimensional representation, often called the latent space or bottleneck layer. The decoder network then takes this low-dimensional representation and reconstructs the original input data.\n",
        "\n",
        "The objective of a vanilla autoencoder is to minimize the reconstruction error between the original input and the output of the decoder. By learning a compressed representation of the input data, autoencoders can capture important features and reduce the dimensionality of the data, allowing for efficient data compression, denoising, or anomaly detection.\n",
        "\n",
        "2. Sparse autoencoders are a variation of autoencoders that introduce a sparsity constraint during training. The sparsity constraint encourages the autoencoder to have only a few active neurons in the latent space, meaning that most neurons have near-zero activations. This promotes a more efficient and compact representation of the input data.\n",
        "\n",
        "The sparsity constraint can be implemented using various techniques such as adding a sparsity penalty term to the loss function, using a specific activation function that induces sparsity (e.g., ReLU), or applying dropout during training.\n",
        "\n",
        "Sparse autoencoders are particularly useful for feature selection, where the sparse latent representations can highlight the most important features in the input data.\n",
        "\n",
        "3. Denoising autoencoders are designed to learn robust representations by training the autoencoder to reconstruct clean input data from corrupted or noisy versions of the data. During training, the input data is intentionally corrupted by adding noise or applying random modifications, and the autoencoder is trained to recover the original, clean data.\n",
        "\n",
        "By forcing the autoencoder to learn to reconstruct the original data from noisy versions, denoising autoencoders can learn more robust and meaningful representations that capture important features and reduce the impact of noise in the input data.\n",
        "\n",
        "4. Convolutional autoencoders are a type of autoencoder architecture that incorporates convolutional layers in both the encoder and decoder networks. These networks are specifically designed for handling high-dimensional structured data such as images.\n",
        "\n",
        "Convolutional autoencoders leverage the convolutional layers to capture spatial dependencies and patterns in the input data. The encoder network applies a series of convolutional and pooling operations to downsample the input image into a compressed representation, while the decoder network uses transposed convolutions (also known as deconvolutions) to upsample the compressed representation and reconstruct the original image.\n",
        "\n",
        "Convolutional autoencoders are widely used in tasks such as image denoising, image compression, and image generation.\n",
        "\n",
        "5. Stacked autoencoders, also known as deep autoencoders or hierarchical autoencoders, are composed of multiple layers of autoencoders stacked on top of each other. Each layer of the stacked autoencoder functions as an encoder and decoder pair, with the output of the previous layer serving as the input to the next layer.\n",
        "\n",
        "By stacking multiple layers, stacked autoencoders can learn hierarchical representations of the input data, with each layer capturing increasingly complex and abstract features. The bottom layers capture low-level features, such as edges or corners, while the top layers capture higher-level features, such as object shapes or textures.\n",
        "\n",
        "Stacked autoencoders are powerful models for unsupervised learning, feature extraction, and representation learning, and they have been successfully used in various domains, including computer vision and natural language processing.\n",
        "\n",
        "6. LSTM autoencoders can be used to generate sentences by leveraging the long short-term memory (LSTM) architecture, which is a type of recurrent neural network (RNN) designed to handle sequence data. LSTM autoencoders encode an input sequence into a fixed-dimensional latent representation and then decode it back into a\n",
        "\n",
        " sequence.\n",
        "\n",
        "To generate sentences using LSTM autoencoders, the encoder LSTM processes the input sentence word by word, updating its hidden state and cell state at each time step. The final hidden state of the encoder LSTM serves as the latent representation. The decoder LSTM takes the latent representation and generates the output sequence word by word, conditioning each word generation on the previous word and the current decoder state.\n",
        "\n",
        "During training, the autoencoder is trained to reconstruct the input sentence. However, during generation, the decoder LSTM can be used to generate new sentences by sampling words from the output distribution at each time step. The generation process can be conditioned on a specific context or can be entirely unconditional, resulting in the generation of diverse sentences.\n",
        "\n",
        "7. Extractive summarization is a text summarization technique that involves selecting and extracting important sentences or phrases from a document to form a concise summary. In extractive summarization, the summary is created by identifying the most relevant and informative parts of the original document without modifying the sentences.\n",
        "\n",
        "Extractive summarization approaches often involve ranking sentences based on their importance scores, which can be computed using various methods. These methods may include statistical measures like term frequency-inverse document frequency (TF-IDF), graph-based algorithms like PageRank, or machine learning techniques such as supervised or unsupervised models.\n",
        "\n",
        "The main advantage of extractive summarization is that it retains the original wording and structure of the selected sentences, ensuring that the summary remains factually accurate and coherent. However, it may not capture the full context or generate novel sentences.\n",
        "\n",
        "8. Abstractive summarization is a text summarization technique that involves generating a summary by understanding the meaning of the source text and expressing it in a concise and coherent manner using new words and sentences. Unlike extractive summarization, abstractive summarization aims to produce summaries that may contain words or phrases not present in the original document.\n",
        "\n",
        "Abstractive summarization approaches typically employ natural language processing and machine learning techniques, such as recurrent neural networks (RNNs), transformer models, or sequence-to-sequence models. These models learn to generate summaries by training on pairs of source documents and corresponding human-generated summaries.\n",
        "\n",
        "The advantage of abstractive summarization is that it can produce more concise and informative summaries by paraphrasing and rephrasing the original content. However, it can be more challenging as it requires understanding the semantics and context of the text and generating coherent and grammatically correct sentences.\n",
        "\n",
        "9. Beam search is a search algorithm used in sequence generation tasks, such as machine translation or text generation, to explore the space of possible output sequences and find the most likely or highest-scoring sequences. It is an extension of the greedy search approach that considers multiple candidates at each decoding step.\n",
        "\n",
        "In beam search, instead of selecting the top-scoring candidate at each time step, a fixed number of candidates, known as the beam size, are kept. The beam size determines the number of hypotheses or sequences that are actively considered during the decoding process. At each step, the beam search expands the search space by considering all possible next words for each candidate and keeps the top-k candidates based on their scores.\n",
        "\n",
        "By considering multiple candidates, beam search allows for exploring different paths in the sequence space and captures a wider range of possibilities. It helps mitigate the risk of getting stuck in local optima and can lead to more diverse and higher-quality output sequences.\n",
        "\n",
        "10. Length normalization is a technique used to adjust the scores of generated sequences based on their length. In many sequence generation tasks, longer sequences tend to have lower probabilities or scores compared to shorter sequences due to the exponential decay of the probabilities.\n",
        "\n",
        "Length normalization addresses this bias by dividing the log-likelihood or score of a generated sequence by its length. This normalization ensures that longer sequences are not penalized and helps prevent shorter sequences from dominating the ranking or scoring process.\n",
        "\n",
        "\n",
        "\n",
        "Length normalization is commonly applied in tasks such as machine translation or text generation to produce more balanced and fair evaluations of generated sequences.\n",
        "\n",
        "11. Coverage normalization is a technique used in abstractive summarization or sequence generation tasks to encourage diversity in the generated output. In abstractive summarization, there is a risk that the generated summary may repeatedly focus on certain parts of the source document while ignoring other important information.\n",
        "\n",
        "Coverage normalization addresses this issue by introducing a coverage vector that keeps track of which parts of the source document have been attended or covered during the generation process. The coverage vector is updated at each decoding step by adding the attention distribution over the source document.\n",
        "\n",
        "During the decoding process, the coverage vector is used to calculate a coverage loss or penalty that encourages the model to cover all parts of the source document. By including the coverage penalty in the overall loss function, the model is incentivized to generate summaries that are more comprehensive and avoid repetition.\n",
        "\n",
        "12. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric used to evaluate the quality of automatic summaries by comparing them to one or more reference summaries. ROUGE measures the overlap and similarity between the generated summary and the reference summaries based on various criteria, including n-gram matching, word sequences, and sequence order.\n",
        "\n",
        "ROUGE computes precision, recall, and F1 scores for different types of n-grams or sequences, such as unigrams, bigrams, or skip-bigrams. It assesses the quality of the summary by considering how well it captures the important content and structure of the reference summaries.\n",
        "\n",
        "ROUGE is commonly used in research and evaluation of summarization systems, and it provides a quantitative measure of the summary's quality compared to the reference summaries."
      ],
      "metadata": {
        "id": "V5JhFRW7a4kP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_JiFavXamhU"
      },
      "outputs": [],
      "source": []
    }
  ]
}