{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX58n9JWLhSZRTNCR2xZ3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/iNeauron_FSDS_Assignments/blob/main/NLP_Assignment/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Sequence-to-sequence models?\n",
        "2. What are the Problem with Vanilla RNNs?\n",
        "3. What is Gradient clipping?\n",
        "4. Explain Attention mechanism\n",
        "5. Explain Conditional random fields (CRFs)\n",
        "6. Explain self-attention\n",
        "7. What is Bahdanau Attention?\n",
        "8. What is a Language Model?\n",
        "9. What is Multi-Head Attention?\n",
        "10. What is Bilingual Evaluation Understudy (BLEU)"
      ],
      "metadata": {
        "id": "vjVnkgzGZBwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer"
      ],
      "metadata": {
        "id": "H8I1ibBsZDnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sequence-to-sequence models, also known as seq2seq models, are a type of neural network architecture that is designed to handle input and output sequences of variable lengths. They consist of two main components: an encoder and a decoder. The encoder takes the input sequence and encodes it into a fixed-length representation or context vector. The decoder then takes this context vector as input and generates the output sequence one step at a time.\n",
        "\n",
        "Seq2seq models are commonly used in various natural language processing tasks such as machine translation, text summarization, and speech recognition. They excel at capturing the dependencies and relationships between different elements in a sequence, allowing them to effectively generate coherent and meaningful output sequences.\n",
        "\n",
        "2. Vanilla RNNs (Recurrent Neural Networks) suffer from a few problems:\n",
        "- Vanishing gradients: When training RNNs with backpropagation, gradients can diminish exponentially over time or vanish, making it challenging for the model to capture long-term dependencies.\n",
        "- Exploding gradients: Conversely, gradients can explode, leading to unstable training and making it difficult to find a good set of model parameters.\n",
        "- Lack of long-term memory: Vanilla RNNs struggle to remember information from earlier time steps when processing long sequences, resulting in a limited ability to capture long-range dependencies.\n",
        "\n",
        "These problems with vanilla RNNs motivated the development of more sophisticated RNN variants like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units), which address these issues and improve the model's ability to handle long-term dependencies.\n",
        "\n",
        "3. Gradient clipping is a technique used during training to mitigate the problem of exploding gradients. When the gradients computed during backpropagation exceed a predefined threshold, they are rescaled or clipped to ensure they do not cause instability or hinder the learning process. By limiting the magnitude of the gradients, gradient clipping helps stabilize the training process and prevents the gradients from becoming too large and leading to unstable updates of the model parameters.\n",
        "\n",
        "4. The attention mechanism is a component used in sequence-to-sequence models to enhance their ability to selectively focus on different parts of the input sequence when generating the output sequence. It provides a way to align and weigh different parts of the input sequence according to their relevance to each step of the output sequence generation.\n",
        "\n",
        "Instead of relying solely on the final context vector produced by the encoder, the attention mechanism allows the decoder to look back at specific parts of the input sequence at each decoding step. It computes attention weights that represent the importance or relevance of each input element, and these weights are used to compute a weighted sum of the input elements. This attention-based information is then combined with the decoder's hidden state to generate the output at each step, allowing the model to focus on different parts of the input sequence dynamically.\n",
        "\n",
        "5. Conditional Random Fields (CRFs) are probabilistic graphical models that are commonly used in sequence labeling tasks, such as named entity recognition, part-of-speech tagging, and speech recognition. CRFs model the conditional probability distribution of the output sequence given the input sequence and learn to assign labels to each element in the input sequence based on its context.\n",
        "\n",
        "Unlike simpler models like hidden Markov models (HMMs), CRFs consider global dependencies between the output labels, taking into account the dependencies among neighboring labels. This global modeling allows CRFs to capture richer interactions and produce more accurate output sequences compared to models that only consider local dependencies.\n",
        "\n",
        "6. Self-attention, also known as intra-attention or scaled dot-product attention, is a mechanism used in transformer-based architectures. It enables the model to capture relationships between different elements within the same input sequence.\n",
        "\n",
        "In self-attention, each element in the input sequence (e.g., a word in a sentence) is associated with three learnable vectors: query, key, and value. The similarity between a query\n",
        "\n",
        " and the keys of all elements is computed, and the resulting similarity scores are used to weight the corresponding values. The weighted values are then summed to obtain the output representation for the query element. This process is performed for every element in the sequence, allowing the model to attend to different parts of the sequence and capture dependencies.\n",
        "\n",
        "Self-attention has proven to be highly effective in tasks involving sequential or hierarchical data, such as machine translation, text classification, and language generation.\n",
        "\n",
        "7. Bahdanau Attention, also known as additive attention, is a specific type of attention mechanism introduced by Dzmitry Bahdanau et al. in the context of neural machine translation. It addresses the limitation of the basic attention mechanism by introducing an additional learnable alignment model.\n",
        "\n",
        "In Bahdanau Attention, the attention weights are computed by a neural network that takes as input the current decoder state and all the encoder hidden states. This alignment model learns to dynamically align different parts of the input sequence with each decoding step. By incorporating this additional alignment model, Bahdanau Attention provides more flexibility and allows the model to capture more nuanced relationships between the input and output sequences.\n",
        "\n",
        "8. A Language Model is a statistical model or a neural network-based model that is trained to predict the probability of a sequence of words or characters in a language. Language models learn the underlying patterns, grammar, and semantics of a language based on the input data they are trained on.\n",
        "\n",
        "The key idea behind language models is to estimate the conditional probability of the next word in a sequence given the previous words. This can be represented as P(w_t | w_1, w_2, ..., w_{t-1}), where w_t represents the t-th word in the sequence. Language models can be trained using various techniques, such as n-gram models, recurrent neural networks (RNNs), or transformer models.\n",
        "\n",
        "Language models find applications in various natural language processing tasks, including machine translation, text generation, speech recognition, and spell checking.\n",
        "\n",
        "9. Multi-Head Attention is a variant of the attention mechanism used in transformer-based architectures. It extends the basic attention mechanism by allowing multiple parallel attention computations, each focusing on different subspaces or aspects of the input sequence.\n",
        "\n",
        "In Multi-Head Attention, the input is projected into multiple query, key, and value vectors, and each head of attention independently attends to these projected vectors. The outputs from each attention head are then concatenated and linearly transformed to obtain the final output of the attention layer. By allowing multiple attention heads, the model can capture different dependencies and relationships between the input and output sequences, enabling it to model complex interactions more effectively.\n",
        "\n",
        "10. Bilingual Evaluation Understudy (BLEU) is an evaluation metric commonly used to measure the quality of machine-generated translations by comparing them to one or more reference translations. BLEU evaluates the similarity between the generated output sequence and the reference sequences based on the n-grams (contiguous sequences of n words) they have in common.\n",
        "\n",
        "BLEU computes a precision score by counting the number of n-grams in the generated output that match with the n-grams in the reference translations. It also considers brevity by penalizing overly short translations. The precision scores for different n-gram orders are combined to compute the BLEU score.\n",
        "\n",
        "BLEU scores range from 0 to 1, with a score of 1 indicating a perfect match with the references. While BLEU is a widely used metric, it has limitations and may not fully capture the semantic quality or fluency of translations."
      ],
      "metadata": {
        "id": "OD6b7lU9ZYHf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slks1fQRY9h-"
      },
      "outputs": [],
      "source": []
    }
  ]
}