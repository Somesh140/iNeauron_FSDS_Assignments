{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWmZVc1Y1eM8SNw3SMYdr6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/iNeauron_FSDS_Assignments/blob/main/NLP_Assignment/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the basic architecture of RNN cell.\n",
        "2. Explain Backpropagation through time (BPTT)\n",
        "3. Explain Vanishing and exploding gradients\n",
        "4. Explain Long short-term memory (LSTM)\n",
        "5. Explain Gated recurrent unit (GRU)\n",
        "6. Explain Peephole LSTM\n",
        "7. Bidirectional RNNs\n",
        "8. Explain the gates of LSTM with equations.\n",
        "9. Explain BiLSTM\n",
        "10. Explain BiGRU"
      ],
      "metadata": {
        "id": "Wy0eBvZZpShQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIQFEfa1nR6A"
      },
      "outputs": [],
      "source": [
        "#Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The basic architecture of an RNN cell consists of three main components: an input, a hidden state, and an output. The input is fed into the cell, and the hidden state from the previous time step is also provided as an input. The cell performs a set of computations, typically using activation functions like the hyperbolic tangent or sigmoid function, and produces an output and an updated hidden state. The updated hidden state is then passed to the next time step, creating a recurrent connection that allows the cell to retain memory of past information.\n",
        "\n",
        "2. Backpropagation through time (BPTT) is a learning algorithm used for training recurrent neural networks. It extends the backpropagation algorithm to handle sequences of data. BPTT unfolds the recurrent network in time, creating a feedforward network where each time step is treated as a separate layer. The error is propagated backward in time, updating the network's weights at each time step. BPTT is used to calculate the gradients of the network's parameters, enabling the learning algorithm to adjust the weights and optimize the network's performance.\n",
        "\n",
        "3. Vanishing and exploding gradients are issues that can occur during the training of deep neural networks, including RNNs. Vanishing gradients refer to the situation where the gradients become extremely small as they propagate backward through the network, making it difficult for the network to learn and update its weights effectively. On the other hand, exploding gradients occur when the gradients become very large, causing unstable learning and convergence. These issues can make training difficult or even impossible in extreme cases.\n",
        "\n",
        "4. Long short-term memory (LSTM) is a type of recurrent neural network architecture designed to address the vanishing gradient problem and capture long-term dependencies in sequential data. LSTM cells contain memory cells and gates that control the flow of information. The memory cells allow the network to retain information over long sequences, while the gates regulate the flow of information through the cell. These gates, such as the forget gate, input gate, and output gate, enable the LSTM to selectively remember or forget information, making it effective in handling long-term dependencies.\n",
        "\n",
        "5. Gated recurrent unit (GRU) is another type of recurrent neural network architecture that addresses the vanishing gradient problem and captures long-term dependencies. Similar to LSTM, GRU also has memory cells and gates, but with a simplified architecture. GRU combines the forget and input gates into a single update gate, and it also has a reset gate that controls the flow of information. This simplified structure makes GRU more computationally efficient and easier to train than LSTM while still capturing important temporal dependencies.\n",
        "\n",
        "6. Peephole LSTM is an extension of the traditional LSTM architecture that includes peephole connections. In addition to the regular input and output gates, peephole connections allow the LSTM cell to observe the cell state when deciding how to update the gates. This allows the cell to have more information about the current cell state, making it better equipped to make decisions during the update process.\n",
        "\n",
        "7. Bidirectional RNNs (BRNNs) are a type of recurrent neural network architecture that processes input data in both the forward and backward directions. In a standard RNN, the information flows from the past to the future, but a BRNN adds an additional set of hidden units that process the data in the reverse order. By considering the context from both directions, BRNNs can capture dependencies and patterns that may be missed by unidirectional RNNs. This makes BRNNs particularly useful in tasks where future and past information is relevant, such as speech recognition or natural language processing.\n",
        "\n",
        "8. The gates of an LSTM are responsible for regulating the flow of information in and out of the memory cell. The equations for the gates are as follows:\n",
        "\n",
        "- Forget Gate (f_t): f_t =\n",
        "\n",
        " sigmoid(W_f * [h_(t-1), x_t] + b_f)\n",
        "- Input Gate (i_t): i_t = sigmoid(W_i * [h_(t-1), x_t] + b_i)\n",
        "- Output Gate (o_t): o_t = sigmoid(W_o * [h_(t-1), x_t] + b_o)\n",
        "\n",
        "Here, h_(t-1) represents the previous hidden state, x_t is the current input, and [h_(t-1), x_t] denotes their concatenation. W_f, W_i, and W_o are weight matrices, and b_f, b_i, and b_o are bias vectors. The sigmoid function is used to ensure that the gate values lie between 0 and 1.\n",
        "\n",
        "9. BiLSTM (Bidirectional LSTM) is a variation of LSTM that incorporates bidirectional processing. It consists of two separate LSTM layers: one processing the input sequence in the forward direction, and the other processing it in the backward direction. The outputs of both LSTM layers are then concatenated or combined to produce the final output. BiLSTM is effective in capturing dependencies in both past and future contexts, allowing the model to make more informed predictions and improve performance in tasks that require a full sequence context.\n",
        "\n",
        "10. BiGRU (Bidirectional Gated Recurrent Unit) is similar to BiLSTM but uses GRU units instead of LSTM units. It operates in a bidirectional manner, with one GRU layer processing the input sequence in the forward direction and another GRU layer processing it in the backward direction. The outputs of both GRU layers are combined to form the final output. BiGRU is useful for capturing dependencies from both past and future contexts, providing a comprehensive understanding of the sequence data and enhancing the model's predictive capabilities."
      ],
      "metadata": {
        "id": "aqIhAUjtpU_I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhUxhT7KqJF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}