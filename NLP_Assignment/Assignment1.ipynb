{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt/AKmjP+aco/GTkdAX7pE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somesh140/iNeauron_FSDS_Assignments/blob/main/NLP_Assignment/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain One-Hot Encoding\n",
        "2. Explain Bag of Words\n",
        "3. Explain Bag of N-Grams\n",
        "4. Explain TF-IDF\n",
        "5. What is OOV problem?\n",
        "6. What are word embeddings?\n",
        "7. Explain Continuous bag of words (CBOW)\n",
        "8. Explain SkipGram\n",
        "9. Explain Glove Embeddings."
      ],
      "metadata": {
        "id": "k-RTP_UD-pb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. One-Hot Encoding is a technique used to represent categorical data as binary vectors where each vector represents a unique category. In this technique, each category is assigned a unique integer index, and then each index is represented as a binary vector with a length equal to the total number of categories. The binary vector contains zeros in all positions except the index of the corresponding category, which is marked as one.\n",
        "\n",
        "2. Bag of Words is a technique used to represent text data as a vector of word frequencies, where each vector represents a document. In this technique, each word in the document is assigned a unique integer index, and the vector is formed by counting the frequency of each word in the document. This approach discards the order of the words in the document and only considers their frequency.\n",
        "\n",
        "3. Bag of N-Grams is a technique used to represent text data as a vector of n-gram frequencies, where each vector represents a document. In this technique, instead of using single words, it considers n-grams, which are contiguous sequences of n words. The vector is formed by counting the frequency of each n-gram in the document. This approach captures some local context and word order information.\n",
        "\n",
        "4. TF-IDF (Term Frequency-Inverse Document Frequency) is a technique used to represent text data as a vector of weighted word frequencies, where each vector represents a document. In this technique, each word in the document is assigned a weight that reflects its importance in the document and across the corpus. The weight is a product of two factors: the term frequency (TF), which measures the frequency of the word in the document, and the inverse document frequency (IDF), which measures the rarity of the word across the corpus.\n",
        "\n",
        "5. OOV (Out-Of-Vocabulary) problem refers to the situation where a word that is present in the test data is not present in the training data, and therefore its representation is missing in the learned model. This problem can lead to inaccurate predictions and reduced performance.\n",
        "\n",
        "6. Word embeddings are dense vector representations of words that capture their semantic and syntactic meanings. Word embeddings are learned by training a neural network on a large corpus of text data, where each word is mapped to a vector in a high-dimensional space. These vectors are learned in such a way that similar words are mapped to nearby points in the space, and dissimilar words are mapped to distant points.\n",
        "\n",
        "7. Continuous bag of words (CBOW) is a neural network architecture used to learn word embeddings. In this architecture, the task is to predict a target word given a context of surrounding words. The input to the network is a bag of word vectors, and the output is a probability distribution over the vocabulary. The network is trained by minimizing the cross-entropy loss between the predicted and actual target word.\n",
        "\n",
        "8. SkipGram is a neural network architecture used to learn word embeddings. In this architecture, the task is to predict the context words given a target word. The input to the network is a single word vector, and the output is a probability distribution over the context words. The network is trained by minimizing the cross-entropy loss between the predicted and actual context words.\n",
        "\n",
        "9. GloVe (Global Vectors) Embeddings is a word embedding method that combines the advantages of count-based and prediction-based approaches. GloVe embeddings are learned by factorizing a co-occurrence matrix that captures the frequency of word co-occurrences in a corpus of text data. The resulting vectors are designed to capture both the global and local context of words and have been shown to outperform other word embedding methods in many natural language processing tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao7N9Nux-rdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFnD9D2s-mvJ"
      },
      "outputs": [],
      "source": []
    }
  ]
}